\section{Metrics for evaluating predictions}

The following metrics can be used to analyze the quality of a \textit{classification model}.

\subsection{Confusion Matrix}

\includegraphics[width=400px]{confusion-matrix.png}

\subsection{Accuracy}

Accuracy answers the question \quotes{\textit{What is the probability that a prediciton is correct?}}.

$$
    Acc = \frac{TP + TN}{TP + TN + FP + FN}
$$

It is only good, if the real distribution of positive and negatives in the data is close to symmetric.

\subsection{Precision}

Precision answers the question \quotes{\textit{If we classify something as positive, how probable is it that it is actually positive?}}.

$$
    Precision = \frac{TP}{TP + FP}
$$

\subsection{Recall}

Recall a.k.a. sensitivity answers the question \quotes{\textit{If a sample is positive, what is the probability we also label it as positive?}}.

$$
    Recall = \frac{TP}{TP + FN}
$$

\subsection{F1 Score}

The F1-score divides the true positives by the sum of the true positives and the mean of the false positives and false negatives. This a high F1-score requires the model to make not few false predictions in either direction. Therefore F1-score is better than accuracy if the real distribution of positive and negative values in the dataset is uneven.

$$
    F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall} = \frac{TP}{TP + \frac{1}{2} \cdot (FP + FN)}
$$


\section{One-hot encoding}

Many machine learning algorithms cannot have a categories as ouput values. Therefore if our goal is a categorization of samples we need to make a transformation of labels to numerical values.

\subsubsection*{Step 1: Integer Transformation}
First we must convert all different variants of the category into distinct integer values, e.g.:\\

\begin{tabular}{l l l}
    Dog         & $\rightarrow$ & 1 \\
    Cat         & $\rightarrow$ & 2 \\
    OtherAnimal & $\rightarrow$ & 3 \\
\end{tabular}

We can now train a machine learning model on that data and it will return one output value. To convert the models output back into classes we could pick the class where the ouput is the closest to the corresponding integer value.

\subsubsection*{One-Hot Encoding}

The problem with integer encoding is that we allow the model to that there is a defined order for the classes. E.g. in the above shown class mapping an ML model could assume that all other animals (3) are closer to cats (2) than to dogs (1). However, as this is not the case at all, using integer encoding might lead to poor predictions.\\
Instead we can use one-hot encoding resolving that issue. For each output class we create an output neuron / node with the value range of $[0, 1]$. The models predictions are converted back to the classes by taking the maximum of all values of the individual classes. The produces output for the individual classes can be seen as a probability that the sample is of instance of the corresponding class.

\begin{tabular}{l l l l l l}
    Dog         & $\rightarrow$ & $[0, 1]$ & $\longrightarrow$ &       \\
    Cat         & $\rightarrow$ & $[0, 1]$ & $\longrightarrow$ & $max$ \\
    OtherAnimal & $\rightarrow$ & $[0, 1]$ & $\longrightarrow$ &       \\
\end{tabular}

\section{Overfitting and Underfitting}

In supervised machine learning we usually have a function that determines a specific target parameter (e.g. if a passenger on the titanic survives). However, the function may not be accurate for all samples since the data may include errors (e.g. measurement errors). As we do not know the real underlying function we try to approximate it with supervised machine learning.\\
If a model is underfitted, it does not take all parameters of the real underlying function into account and therefore is not accurate.\\
If a model is overfitted, it takes parameters into account that do not apply to the underlying function but where given in the training samples. Overfitting leads to bad results on unseen data as parameters have been learned that are no general indicators for an event and can not be used on other data than the training samples. Overfitting may also occur if the paramters are tuned in a too detailed way that is only applicable to errors or inaccuracy of measurements in the training data.

\subsection{How can it be detected?}

To detect underfitting and overfitting we can validate the model after each training epoch by letting it make predictions on unseen data and calculating a chosen metric (in the figure we calculate a loss) over that data. Now we can make the following interpretations of the graph:

\begin{enumerate}
    \item As long as the loss is decreasing the model is still underfitted and shall be trained for more epochs.
    \item If the los is increasing the model is becoming more and more overfitted and training can be stopped.
    \item If the loss remains unchanged the model has reached the global or a local optimum.
\end{enumerate}

\includegraphics[width=400px]{underfitting-overfitting.png}

\subsection{Possible solutions}

\begin{enumerate}
    \item \textbf{Stop early:}\\
          Stop training to avoid overfitting if your score on the validation set does not increase anymore.
    \item \textbf{k-fold-cross-validation:}\\
          Split dataset into k groups and train k-times on k-1 groups while using the remaining group as validation data. This way after each epoch we train on data that has not been seen in that epoch while training more than one epoch.
    \item \textbf{Increase dataset size:}\\
          This has positive influence on underfitting and overfitting.
    \item \textbf{Data augmentation:}\\
          Create more data by applying some tranformations to the samples. E.g. flip and crop images. This helps preventing that the model learns too much details about individual samples.
    \item \textbf{Reduce Complexity:}\\
          Use pooling layers and less neurons to decrease overfitting.
    \item \textbf{Regularizaiton:}\\
          Penalizing big numbers of coefficients.
    \item \textbf{Use Ensembling:}\\
          Combine predictions of multiple ML-models.
\end{enumerate}

\section{PCA - principal component analysis}

\subsection{Reasons for using PCA}

If we have data with $n$ variables/features we can use principal component analysis to reduce the amount of features to $k$ with $k < n$ features while keeping the most important features of the data and eliminating the less important features. Reduction of features is useful to plot the data (because plots with more than 3 features are non-trivial) and also for machine-learning models to increase computation speed.

\subsection{Algorithm}
\begin{enumerate}
    \item Calculate mean for each variable. Doing that we also get the center of the data as $(mean(x_{1}), mean(x_{2}), ... mean(x_{n}))$.
    \item Now we want to shift the data so that its center is at the center of the coordinate system. We can subtract each individual mean from the corresponding variable to do that.
    \item We now compute the principle component 1 PC1. We are searching for a vector (straight line) that fits the data in the $x_{1}$ axis best. That means we search for a line where the squared distances of the data points to this line are minimal. It is important that this is equivalent to finding a line where the data is spread out the most if we project it onto the line.
    \item For each other variable $v_{2}$ to $v_{n}$ we draw a line that is orthogonal to all preceding lines and rotate it until it represents the data best for the given variable in the same way as with the first line. These lines are $PC_{2}$ to $PC_{n}$.
    \item As said, the data points of the features are spread out as much as possible along the PCs now. This means that the variance and therefore also the amount of information is maximized for the specific variable if we project the data onto that PC. Now we can take the $k$ PCs with the greatest variance (also called eigenvalues here) and use them to represent our data.
\end{enumerate}

\section{Python Basics}

\subsection{Slicing}

\begin{lstlisting}
    a[start:stop]       # items start through stop-1
    a[start:]           # items start through the rest of the array
    a[:stop]            # items from the beginning through stop-1
    a[:]                # a copy of the whole array
    a[start:stop:step]  # start through not past stop, by step
\end{lstlisting}

\subsection{Data Extraction with Pandas}

\begin{lstlisting}
    df.head(5)                          # show first 5 lines
    df.tail(3)                          # show last 3 lines
    df.columns              
    df.describe()                       # statistic summary of data
    df["Survived"]                      # get survived column as pandas.Series
    df[0:3]                             # get the first 3 rows with all columns
    df.loc["2013-01-03"]                # Select a row by the value of the index column
    df.loc["2013-01-03", "name"]        # Select the value of the `name` column of that row
    df.iloc[5]                          # Select the 5th row by its index
    df[df["income"] > 1000]             # selecting rows via a boolean array
    df.dropna(how="any")                # drop all rows that contain null values
    df.mean()                           # calculates mean of each column and returns a Series
    df1.fillna(value=5)                 # replace all NaN values with 5
    df.apply(lambda x: x.max() - 10)    # apply a function to each data point
\end{lstlisting}


\section{Regularization}

\subsection{What is regularization}
Regularization is the method of penalizing complex model e.g. models with a lot of parameters. It is used to avoid overfitting.\\

A common loss function for evaluating a model is the residual sum of squares ($RSS$).

$$
    RSS = \sum_{i=1}^{n} (y_{i} - y'_{i})^{2}
$$

When evaluating the model the loss function shall be minimized.\\
But we can also use other loss functions that include regularization terms as will be shown in the following.

\subsection{Ridge}

Ridge adds the sum of the squares of coefficients $w$ to the RSS which makes the loss function prefer smaller coefficients. However coefficients will never be zeroed out using Ridge.

$$
    Ridge = RSS + \lambda \sum_{i=0}^{n} w_{i}^{2}
$$

\subsection{Lasso}

Lasso adds the sum of the absolutes of coefficients $w$ to the RSS which makes the loss function prefer smaller coefficients and possibly drive some coefficients to zero i.e. eliminating them.

$$
    Ridge = RSS + \lambda \sum_{i=0}^{n} |w_{i}|
$$


\subsection{Dropout}

As opposed to Ridge and Lasso, which are used mainly used with linear models, dropout is a regularization method used for neural networks. Using dropout means that the output values of some randomly chosen nodes of a layer (which may not be the output-layer) are ignored. This way the net is meant to be morel robust to noise in the training data, because it the dropped out nodes will be different in every epoch and therefore the training experience will also slightly differ.

\section{Machine Learning Tasks}

\subsection{Classification}

Classification is a \textit{supervised} machine learning task for predicting a target variable wich may have a finite number of possible values.

\subsection{Regression}

Regression is a \textit{supervised} machine learning task for predicting a continuous target variable like the price of a house or the height of a human.

\subsection{Clustering}

Clustering is an \textit{unsupervised} machine learning task for grouping data points to multiple clusters. The data points in one cluster shall be as similar as possible to each other while being as dissimilar as possible to data points in other clusters.

\section{MLP - Multi Layer Perceptron}

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{MLP.png}
    \caption{An MLP with one hidden layer.}
    \label{fig:mlp}
\end{figure}

\subsection{What is an MPL?}

An MPL - multi layer perceptron - is a type of feed-forward artificial neural network (ANN) consisting of more multiple layers of perceptrons.\\
A perceptron is a single artificial neuron. It has a finite number of inputs with associated weights and one output. The output for a single perceptron is defined by the sum of all weights combined with an \hyperref[sec:activation_functions]{activation function}. Single perceptrons can only model linear functions (however with regard to multiple variables (inputs)) as the output is calculated as a polynomial of degree 1 of all inputs. However, with multiple layers of perceptrons we can also model non-linear functions, which is shown on the example of XOR in \hyperref[sec:solving_non_linear_problems_with_nns]{section \ref*{sec:solving_non_linear_problems_with_nns}}.

Note: You can also refer to an MLP as an ANN which has the following properties:
\begin{enumerate}
    \item Amount of layers $> 1$.
    \item All layers are fully connected layers.
\end{enumerate}

\subsection{Number of parameters in MPLs}

A parameter in a multi layer perceptron is one of the weights that are changed during training visualized by the lines between neurons.\\

We declare the following variables:
\begin{itemize}
    \item $l_{i}$: amount of neurons in layer $i$.
\end{itemize}

If $l_{i}$ is the amount of neurons in layer $i$ starting with the input layer being layer $1$, then the number of parameters in an MLP without biases is:

$$
    \sum_{i=1}^{n-1}l_{i}\cdot{l_{i+1}}
$$

If the MLP has biases in every layer, the amount of biases is calculated as the sum of neurons in all layers except the input layer. This is because as \hyperref[fig:mlp]{figure \ref*{fig:mlp}} shows, biases are usually connected to every neuron of a layer and input layers have no biases:

$$
    \sum_{i=2}^{n}l_{i}
$$

Therefore when calculating the amount of parameters in an MPL with biases in every layer, of course add up both terms.

\section{Feature map calculation in convolutional NN}

TODO: do this

\section{Input and output sizes in Neural networks}

TODO: do this

Describe here: Size of inputs and outputs in MLP and convolutional NN calculated from image size and the number of output classes.


\section{Activation Functions}
\label{sec:activation_functions}

An activation function is a function that is applied to the output of each node of a ANN. A simple activation function could be one that puts out either $0$ or $1$ depending on a threshold. Three more activation functions shall be presented in the following.

\subsection{ReLU}

TODO: explain why this is used

$$
    relu(x) =
    \begin{cases}
        \begin{rcases*}
            x & \quad x $>$ 0, \\
            0 & \quad x $\leq$ 0 \\
        \end{rcases*}
    \end{cases}
$$

\begin{figure}[h]
    \centering
    \includegraphics[width=150px]{ReLU.png}
    \caption{ReLU activation function}
    \label{fig:relu}
\end{figure}

\subsection{Sigmoid}

TODO: explain why this is used

$$
    sigmoid(x) = \frac{1}{1 + e^{-z}}
$$

\begin{figure}[h]
    \centering
    \includegraphics[width=150px]{sigmoid.png}
    \caption{Sigmoid activation function}
    \label{fig:sigmoid}
\end{figure}

\subsection{Softmax}

$$
    softmax(x)_{j} = \frac{e^{x}}{\sum_{k=1}^{K}e_{k}^{x}}
$$


The softmax function is a function that is typically applied to all output values of a NN for a classification problem. We assume that we have $K$ classes (output neurons) and an input $x$. The softmax function transforms the output values of each $k$-th neuron in a way that they all together add up to one and can therefore seen as probabilities for the corresponding class.

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{softmax.png}
    \caption{Softmax activation function}
    \label{fig:softmax}
\end{figure}

\section{Solving non-linear problems with NNs}

\label{sec:solving_non_linear_problems_with_nns}

Use example of logical function XOR here.

\section{K-means}

\section{Gradient Descent}

\section{Hyperparameters of ML models}

\subsection{Learning Rate}
\subsection{Epochs}
\subsection{Regularization}
\subsection{Batch Size}
\subsection{Convolution Kernel size}
\subsection{Max-Pooling}

\section{Logistic Regression and Cross Entropy}


\section{Linear Regression and Normal Equation}


\section{Decision Trees}


\section{K-nearest Neighbors}

